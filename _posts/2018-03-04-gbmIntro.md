---
layout: post
mathjax: true
title: Gradient Boosting 原理介绍
categories: [机器学习, 集成学习]
description: 介绍梯度提升算法的原理、特点
keywords: 梯度提升, 集成学习
---

集成学习通过构建并结合多个学习器来完成学习任务。通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。集成学习的一般做法：先产生一组“个体学习器”，再用某种策略将它们结合起来。常见的集成策略有：Bagging、Boosting和Stacking。本文介绍Boosting的代表算法Gradient Boosting。

Boosting算法的主要思路：依次生成一个基学习器。每次迭代，忽略先前已生成的基学习器已经处理得很好的样本，重视之前的基学习器难以学习的样本，训练得到下一个基学习器。进行多次迭代，直到基学习器数目达到事先指定的值T，最后将T个基学习器进行加权结合。

Boosting的代表算法有：Adaboost和Gradient Boosting。Adaboost在每次迭代时，更新每个训练样本的权重，对难以正确分类的样本施加更多的权重，对已经分类正确的样本，减少其权重。新的基学习器基于更新权重的训练样本进行训练学习，使得新的基学习器更重视难以正确分类的样本。Gradient Boosting则将Boosting看作是一个数值优化问题，通过类似于梯度下降的步骤，依次地添加基学习器，以最小化损失函数为目标，在函数空间（假设集）里寻找一个最优函数。

### 1、梯度提升

#### 1.1 梯度下降

梯度下降是机器学习用于求解模型参数的常用方法。对于这样的典型优化问题：find $$\hat{x}=\operatorname*{arg\,max}_x f(x)$$，通常采用梯度下降求解，算法流程如下：

* 给定一个起始点

* 对于 $$i=1,2,\dotsc,K$$ 分别做如下迭代：

  (a)  $$x_i=x_{i-1}+\gamma_{i-1}* g_{i-1}$$，这里的 $$g_{i-1}=-\left.\frac{\partial f}{\partial x}\right|_{x=x_{i-1}}$$ 表示 $$f$$ 在 $$x_{i-1}$$ 点的梯度

* 直到 $$|g_{i-1}|$$ 足够小，或者 $$|x_i - x_{i-1}|$$ 足够小，或者迭代次数达到指定值 $$T$$

在参数空间里寻找最优参数，以实现最小化目标函数的目标。整个寻优过程就是小步快跑的过程，每次都往目标函数下降最快的那个方向前进一小步。最后，得到的最优参数表示为加和形式：
$$
x_k=x_0+\gamma * g_1+\gamma_2 * g_2+\dotsb+\gamma_k*g_k
$$

#### 1.2 Gradient Boosting

Gradient Boosting 由梯度下降启发而来。梯度下降是在参数空间里寻找一个最优点，Gradient Boosting可以理解为在函数空间（假设集）寻找一个最优函数。优化的目标通常是通过一个损失函数来定义：
$$
\operatorname*{arg\,min}_F L(F) = \operatorname*{arg\,min}_F\sideset{}{_{i=0}^N}\sum\text{Loss}(F(x_i),y_i)
$$
常见的损失函数如平方差函数：$$\text{Loss}(F(x_i),y_i)=(F(x_i)-y_i)^2$$

我们采用类似梯度下降的方法，进行多次迭代，通过求取梯度的方式，依次构造基学习器$$f_1,f_2,f_3,\dotsc,f_m$$ 。

最后得到的函数的形式为：$$F_m(x)=f_0+\alpha_1f_1(x)+\alpha_2f_2(x)+\alpha_3f_3(x)+\dotsb+\alpha_mf_m(x)​$$

每次求取梯度时，我们对损失函数 $$L$$，以 $$F$$ 为参考求取梯度：$$g_i=-\left.\frac{\partial L}{\partial F}\right|_{F=F_{i-1}}$$ 。这里的 $$F_{i-1}$$ 是截至前一次迭代，求得的F函数。

一个函数对函数的求导不好理解，通常无法通过上述的公式直接求解到梯度函数 $$g_i$$ 。但是，对于每个训练样本，我们可以得到梯度 $$g_i$$ 在该训练样本上的值：

$$
\hat{g_i}(x_k)=-\left.\frac{\partial L}{\partial F(x_k)}\right|_{F=F_{i-1}}\text{for }k= 1,2,3,\dotsc,N
$$

严格来说 $$\hat{g_i}(x_k)\text{ for }k=1,2,\dotsb,N$$ 只是描述了 $$g_i$$ 在某些个别点上的值，并不足以表达 $$g_i$$ ，但是但是我们可以通过函数拟合的方法从 $$\hat{g_i}(x_k)\text{ for }k=1,2,\dotsb,N$$ 构造 $$g_i$$ ，这样我们就通过近似的方法得到了函数对函数的梯度求导。

依次，Gradient Boosting 算法的过程可以总结如下：

* ​



